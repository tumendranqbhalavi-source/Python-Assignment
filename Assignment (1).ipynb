{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRayeNHUcjWzKlKSHheJcc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment Question"],"metadata":{"id":"CkEdlnUgoOof"}},{"cell_type":"markdown","source":["# ***1. What is a parameter***"],"metadata":{"id":"jWQ5VDBnoULW"}},{"cell_type":"markdown","source":["In **Machine Learning**, a **parameter** is a **variable that the model learns automatically during training**. These parameters define how the model makes predictions and are adjusted to minimize errors and improve accuracy. In simple terms, parameters are the **internal settings** that control the behavior of the model.\n","\n","For example:\n","\n","* In **Linear Regression**, the parameters are the **slope (weights)** and **intercept (bias)** of the line.\n","* In **Neural Networks**, parameters include the **weights and biases** of the connections between neurons.\n","\n","During the training process, algorithms adjust these parameters using optimization techniques (like **gradient descent**) so that the model’s predictions get closer to the actual results.\n","\n","In short, **parameters are the learned values inside a machine learning model that determine how it processes input data and produces outputs.**\n"],"metadata":{"id":"W67oW24hoXu5"}},{"cell_type":"markdown","source":["# ***2. What is correlation? What does negative correlation mean***"],"metadata":{"id":"6tbEtaHvoemy"}},{"cell_type":"markdown","source":["Ans= **Correlation** is a statistical term that describes the **relationship between two variables** — how one variable changes when the other changes. It tells us whether the variables move in the same direction, opposite directions, or have no relationship at all. The strength and direction of correlation are usually measured using a value called the **correlation coefficient**, which ranges from **-1 to +1**.\n","\n","A **negative correlation** means that when one variable increases, the other variable tends to decrease. In other words, they move in **opposite directions**. For example, if the amount of exercise a person does increases and their body weight decreases, it shows a negative correlation between exercise and weight. The closer the correlation value is to **-1**, the stronger the negative relationship between the two variables.\n"],"metadata":{"id":"EE-d8yRlor9U"}},{"cell_type":"markdown","source":["# ***3. Define Machine Learning. What are the main components in Machine Learning***"],"metadata":{"id":"yLSHg3T1ovMZ"}},{"cell_type":"markdown","source":["Ans= Machine Learning is a branch of Artificial Intelligence (AI) that enables computers to learn from data and improve their performance automatically without being explicitly programmed. Instead of following fixed instructions, a machine learning model uses algorithms to analyze data, recognize patterns, and make predictions or decisions based on what it has learned.\n","\n","The main components of Machine Learning are:\n","\n","Data – The most important part; it is the raw information (numbers, text, images, etc.) used to train the model.\n","\n","Model – A mathematical representation that learns patterns from the data and makes predictions or classifications.\n","\n","Algorithm – The method or process used to train the model, such as Linear Regression, Decision Tree, or Neural Network.\n","\n","Training – The process where the model learns by adjusting itself based on the input data and its outputs.\n","\n","Evaluation – Checking how well the model performs using test data to measure accuracy and efficiency.\n","\n","Prediction – The final step where the trained model is used to make decisions or forecasts on new, unseen data."],"metadata":{"id":"yMjaaa62o6qO"}},{"cell_type":"markdown","source":["# ***4. How does loss value help in determining whether the model is good or not***"],"metadata":{"id":"GTAGUYuSpA2C"}},{"cell_type":"markdown","source":["Ans= The **loss value** in Machine Learning is a numerical measure that shows **how well or poorly a model is performing**. It represents the **difference between the model’s predicted output and the actual (true) output**. A smaller loss value means the model’s predictions are closer to the real values, while a larger loss value means the predictions are far off.\n","\n","In simple terms, the **loss function** acts like a teacher — it tells the model how “wrong” it is after each prediction. During training, the model continuously adjusts its parameters to **minimize the loss value**, trying to make its predictions more accurate.\n","\n","So, if the **loss value is low**, it indicates that the model is performing well and making accurate predictions. But if the **loss value is high**, it means the model is not learning properly and needs improvement — such as more data, better features, or tuning of parameters.\n"],"metadata":{"id":"vSFOKaoOpHai"}},{"cell_type":"markdown","source":["# ***5. What are continuous and categorical variables***"],"metadata":{"id":"pTUUQkpGpMsx"}},{"cell_type":"markdown","source":["Ans= In statistics and machine learning, **variables** are the features or characteristics that hold data values. These variables are generally divided into two main types — **continuous** and **categorical** variables.\n","\n","A **continuous variable** is one that can take **any numerical value within a given range**. It can be measured and often includes decimal points. Examples include height, weight, temperature, or time — since these values can vary smoothly and continuously. For instance, a person’s height can be 165.4 cm or 165.45 cm; there is no fixed gap between values.\n","\n","On the other hand, a **categorical variable** represents **distinct groups or categories** and usually takes on a limited number of possible values. These are often labels or names rather than numbers. Examples include gender (male/female), color (red/blue/green), or type of car (sedan/SUV/truck). Categorical variables describe **qualities or characteristics**, not quantities.\n","\n"],"metadata":{"id":"__HnteMepSEt"}},{"cell_type":"markdown","source":["# ***6. How do we handle categorical variables in Machine Learning? What are the common t echniques***"],"metadata":{"id":"5KGfU8e_pZdy"}},{"cell_type":"markdown","source":["Ans= In Machine Learning, **categorical variables** need to be converted into a **numerical format** because most algorithms can only process numerical data. This process is called **encoding**. Handling categorical variables properly is important because it ensures the model can correctly understand and learn from the data.\n","\n","Here are the **common techniques** used to handle categorical variables:\n","\n","1. **Label Encoding** – This technique assigns a **unique number to each category**. For example, “Red,” “Blue,” and “Green” can be encoded as 0, 1, and 2. It is simple but best for **ordinal data** (where order matters, like “low,” “medium,” “high”).\n","\n","2. **One-Hot Encoding** – In this method, each category is converted into a **separate binary column** (0 or 1). For example, “Red,” “Blue,” and “Green” become three columns where only one is “1” at a time. It is used for **nominal data** (where order doesn’t matter).\n","\n","3. **Ordinal Encoding** – Similar to label encoding, but it specifically respects the **order or ranking** among categories (like “small,” “medium,” “large”).\n","\n","4. **Target Encoding (Mean Encoding)** – Each category is replaced with the **mean value of the target variable** for that category. It is used in advanced models but must be applied carefully to avoid overfitting.\n","\n","In summary, handling categorical variables involves **converting text labels into numerical values** using encoding techniques like **Label Encoding** and **One-Hot Encoding**, so that machine learning algorithms can process and learn from them effectively.\n"],"metadata":{"id":"wLL3g6Cqph-g"}},{"cell_type":"markdown","source":["# ***7. What do you mean by training and testing a dataset***"],"metadata":{"id":"flW3qSz5pnVN"}},{"cell_type":"markdown","source":["Ans= In Machine Learning, **training and testing a dataset** are two important steps used to build and evaluate a model’s performance.\n","\n","**Training a dataset** means using a portion of the data to **teach the model**. During training, the model learns patterns, relationships, and rules from the input data so it can make predictions. The dataset used for this step is called the **training set**, and it helps the model adjust its parameters to minimize errors.\n","\n","**Testing a dataset**, on the other hand, means using a **separate portion of the data** (that the model has never seen before) to **check how well the model performs**. This dataset is called the **testing set**, and it helps us measure how accurately the model can make predictions on new, unseen data.\n","\n","In simple terms, **training** is like studying from a textbook, and **testing** is like taking an exam to see how well you’ve learned. This process ensures that the model doesn’t just memorize the data but can **generalize** its learning to make accurate predictions in real-world situations.\n"],"metadata":{"id":"6CBhxf5-ptSE"}},{"cell_type":"markdown","source":["# ***8. What is sklearn.preprocessing***"],"metadata":{"id":"XcONWH9fpzVQ"}},{"cell_type":"markdown","source":["Ans= In Python’s Machine Learning library **scikit-learn (sklearn)**, the module **`sklearn.preprocessing`** is used for **preprocessing and transforming raw data** before feeding it into a model. Real-world data often contains values in different scales, formats, or even missing entries, and preprocessing helps make the data clean, consistent, and suitable for machine learning algorithms.\n","\n","The **`sklearn.preprocessing`** module provides several tools for common tasks such as **scaling, normalization, encoding categorical data, and handling missing values**. For example, it includes classes like `StandardScaler` for scaling numerical data, `MinMaxScaler` for normalization, and `LabelEncoder` or `OneHotEncoder` for converting categorical variables into numerical form.\n","\n","In short, **`sklearn.preprocessing` helps prepare raw data into a structured, numerical format** so that machine learning models can learn efficiently and give more accurate results.\n"],"metadata":{"id":"aROaFyNxqIzr"}},{"cell_type":"markdown","source":["# ***9. What is a Test set***"],"metadata":{"id":"eWiwmyyRqKJ3"}},{"cell_type":"markdown","source":["In Machine Learning, a **test set** is a **portion of the dataset that is kept separate from the training data** and is used to **evaluate the performance of a trained model**. Unlike the training set, which the model uses to learn patterns and adjust its parameters, the test set contains **new, unseen data** that the model has never encountered before.\n","\n","The main purpose of the test set is to check how well the model can **generalize** — that is, make accurate predictions on data outside of what it was trained on. By comparing the model’s predictions on the test set with the actual values, we can measure its **accuracy, error, or other performance metrics**.\n","\n"],"metadata":{"id":"ufbsAv4CqRFW"}},{"cell_type":"markdown","source":["# ***10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem***"],"metadata":{"id":"6NfFicYeqTz-"}},{"cell_type":"markdown","source":["Ans= 1. Splitting data for training and testing in Python\n","\n","In Python, we usually use train_test_split from scikit-learn to divide a dataset into a training set and a testing set. This ensures the model can learn from one part of the data and be evaluated on unseen data.\n","\n","2. Approach to solving a Machine Learning problem\n","\n","A structured approach usually involves the following steps:\n","\n","Understand the problem – Define the objective and type of problem (classification, regression, clustering, etc.).\n","\n","Collect and explore data – Gather the dataset and perform exploratory data analysis (EDA) to understand patterns, distributions, and missing values.\n","\n","Preprocess data – Handle missing values, encode categorical variables, normalize or scale features, and remove irrelevant data.\n","\n","Split the data – Divide the dataset into training and testing sets.\n","\n","Select a model – Choose an appropriate algorithm depending on the problem (e.g., Linear Regression, Decision Tree, SVM).\n","\n","Train the model – Fit the model on the training data and adjust parameters.\n","\n","Evaluate the model – Test the model on the test set using metrics like accuracy, RMSE, precision, recall, or F1-score.\n","\n","Tune the model – Optimize hyperparameters, try different algorithms, or improve feature engineering to get better performance.\n","\n","Deploy the model – Use the trained model to make predictions on real-world data.\n","\n","Monitor and update – Continuously monitor the model’s performance and retrain with new data if necessary."],"metadata":{"id":"h9kgen9oqc4N"}},{"cell_type":"markdown","source":["# ***11. Why do we have to perform EDA before fitting a model to the data***"],"metadata":{"id":"stWa9nKMqq97"}},{"cell_type":"markdown","source":["Ans= We perform Exploratory Data Analysis (EDA) before fitting a Machine Learning model because it helps us understand the data deeply and ensures the model can learn effectively. EDA is like getting to know the dataset before making any decisions. Here’s why it’s important:\n","\n","Identify patterns and relationships – EDA helps reveal how features relate to each other and to the target variable. This understanding can guide feature selection and model choice.\n","\n","Detect missing or inconsistent data – Real-world datasets often have missing, duplicate, or incorrect values. EDA helps us spot and handle these issues before training the model.\n","\n","Understand data distribution – Knowing whether a variable is normally distributed, skewed, or contains outliers helps in deciding preprocessing steps like scaling, normalization, or transformations.\n","\n","Detect outliers – Extreme values can negatively impact certain models (like linear regression). EDA helps identify and handle outliers properly.\n","\n","Choose the right model and features – By exploring correlations and patterns, we can select the most important features and avoid irrelevant ones, which improves model performance.\n","\n","Prevent data leakage – EDA helps ensure that no information from the test set or target variable is inadvertently used during training, which could give misleadingly high performance."],"metadata":{"id":"z6qMRkoEqwTC"}},{"cell_type":"markdown","source":["# ***12. What is correlation***"],"metadata":{"id":"mbtV7WbAq25F"}},{"cell_type":"markdown","source":["Ans= **Correlation** is a statistical measure that describes the **relationship between two variables** — how one variable changes when the other changes. It indicates whether the variables move **in the same direction, in opposite directions, or are independent**. The strength and direction of correlation are usually represented by the **correlation coefficient**, which ranges from **-1 to +1**. A positive correlation means that as one variable increases, the other also increases, while a negative correlation means that as one variable increases, the other decreases. A correlation close to 0 indicates **no linear relationship** between the variables. Correlation is widely used in data analysis and machine learning to understand relationships between features and to select important variables for modeling.\n"],"metadata":{"id":"mAnCE083rGci"}},{"cell_type":"markdown","source":["# ***13. What does negative correlation mean***"],"metadata":{"id":"nabRtTdmrIOm"}},{"cell_type":"markdown","source":["Ans= **Negative correlation** means that two variables are **inversely related** — when one variable increases, the other variable tends to decrease. In other words, they move in **opposite directions**. The strength of this relationship is measured by the **correlation coefficient**, which ranges from **-1 to 0** for negative correlation. A value close to **-1** indicates a strong negative relationship, while a value near 0 indicates a weak or no linear relationship. For example, if the number of hours spent watching TV increases and exam scores decrease, there is a negative correlation between TV time and performance. Negative correlation helps in understanding relationships where increases in one factor lead to decreases in another.\n"],"metadata":{"id":"ZYS4h0n7rPLB"}},{"cell_type":"markdown","source":["# ***14. How can you find correlation between variables in Python***"],"metadata":{"id":"m1c1uH6qrRvs"}},{"cell_type":"markdown","source":["Ans= In Python, you can find the correlation between variables using libraries like Pandas or NumPy, which provide built-in functions to calculate correlation coefficients. The most common method is the Pearson correlation, which measures the linear relationship between two variables.\n","\n","For example, using Pandas:"],"metadata":{"id":"r263zTdFrWvB"}},{"cell_type":"code","source":["import pandas as pd\n","\n","data = {'X': [1, 2, 3, 4, 5],\n","        'Y': [2, 4, 6, 8, 10]}\n","\n","df = pd.DataFrame(data)\n","\n","correlation = df.corr()\n","print(correlation)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jYivNUhrkdU","executionInfo":{"status":"ok","timestamp":1760931254065,"user_tz":-330,"elapsed":1857,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"52378965-3dc2-4edb-b0fc-e8dcba964d89"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["     X    Y\n","X  1.0  1.0\n","Y  1.0  1.0\n"]}]},{"cell_type":"markdown","source":["# ***15. What is causation? Explain difference between correlation and causation with an example***"],"metadata":{"id":"5xICfvVIrrAd"}},{"cell_type":"markdown","source":["Ans= **Causation** refers to a relationship between two variables where **one variable directly affects or causes a change in the other**. In other words, a change in the cause variable leads to a predictable change in the effect variable.\n","\n","The key difference between **correlation** and **causation** is that:\n","\n","* **Correlation** indicates that two variables **move together** (either in the same or opposite directions), but it does **not imply that one causes the other**.\n","* **Causation** implies a **direct cause-and-effect relationship**, where changes in one variable result in changes in the other.\n","\n","**Example:**\n","\n","* Correlation: Ice cream sales and drowning incidents are positively correlated — both increase during summer. However, buying ice cream does **not cause** drowning; the correlation exists because of a **hidden factor** (hot weather).\n","* Causation: Smoking and lung cancer have a causal relationship — smoking **directly increases the risk** of lung cancer.\n","\n","\n"],"metadata":{"id":"mazw3XfJr4EQ"}},{"cell_type":"markdown","source":["# ***16. What is an Optimizer? What are different types of optimizers? Explain each with an example***"],"metadata":{"id":"hootoVZ6r89o"}},{"cell_type":"markdown","source":["Ans=In Machine Learning and Deep Learning, an optimizer is an algorithm used to update the model’s parameters (like weights and biases) during training in order to minimize the loss function. The optimizer controls how the model learns from the data by adjusting these parameters step by step to reduce errors and improve accuracy. Choosing the right optimizer is crucial because it affects the speed of convergence and the model’s final performance.\n","\n","There are several types of optimizers commonly used:\n","\n","Gradient Descent (GD) – This is the simplest optimizer, where the model updates its parameters by moving in the opposite direction of the gradient of the loss function. It can be Batch Gradient Descent, where all training examples are used to compute the gradient at once.\n","Example:\n","If a linear regression model predicts y_pred and the loss is Mean Squared Error (MSE), gradient descent updates weights w using:\n","w = w - learning_rate * gradient.\n","\n","Stochastic Gradient Descent (SGD) – Instead of using the entire dataset, SGD updates parameters for each training example individually. This introduces randomness, which can help escape local minima and speed up training.\n","Example: Training a neural network on 10,000 images by updating weights after processing each image rather than all 10,000 at once.\n","\n","Mini-Batch Gradient Descent – A compromise between GD and SGD, it updates parameters using small batches of data at a time. This is faster than full-batch GD and more stable than SGD.\n","Example: Using a batch size of 32 images to update a convolutional neural network during training.\n","\n","Momentum – Momentum optimizer adds a fraction of the previous update to the current update, helping accelerate convergence and reducing oscillations in regions with steep slopes.\n","Example: If the previous weight update was 0.1 and the current gradient suggests 0.05, the new update could be 0.1*momentum + 0.05.\n","\n","RMSprop (Root Mean Square Propagation) – RMSprop adjusts the learning rate individually for each parameter by dividing the learning rate by the moving average of recent gradients squared. This helps in handling vanishing or exploding gradients.\n","Example: Commonly used in training recurrent neural networks (RNNs) for sequence prediction.\n","\n","Adam (Adaptive Moment Estimation) – Adam combines Momentum and RMSprop by keeping track of both the first moment (mean) and second moment (variance) of gradients. It adapts the learning rate for each parameter, making it fast and effective for most deep learning tasks.\n","Example: Training complex neural networks for image classification or natural language processing tasks"],"metadata":{"id":"TQhIAdfGsfws"}},{"cell_type":"markdown","source":["# 17. What is sklearn.linear_model"],"metadata":{"id":"MAs0eaB7soOx"}},{"cell_type":"markdown","source":["Ans= In Python, sklearn.linear_model is a module in the scikit-learn library that provides classes and functions for linear models. Linear models are a group of algorithms that assume a linear relationship between input features and the target variable. This module is widely used for both regression and classification tasks because of its simplicity, interpretability, and efficiency.\n","\n","The module includes several important classes, such as:\n","\n","LinearRegression – Used for predicting a continuous target variable based on one or more input features. It fits a line (or hyperplane in multiple dimensions) that minimizes the difference between predicted and actual values.\n","Example: Predicting house prices based on area and number of rooms.\n","\n","LogisticRegression – Used for classification problems where the target variable is categorical. It predicts the probability of an event occurring and applies a sigmoid function to map predictions between 0 and 1.\n","Example: Predicting whether a customer will buy a product (yes/no).\n","\n","Ridge – A type of linear regression that adds L2 regularization to prevent overfitting by penalizing large coefficients.\n","Example: Predicting sales while controlling for overly influential features.\n","\n","Lasso – Linear regression with L1 regularization, which can shrink some coefficients to zero, effectively performing feature selection.\n","Example: Selecting the most important features in predicting stock prices.\n","\n","ElasticNet – Combines both L1 and L2 regularization to balance feature selection and coefficient shrinkage."],"metadata":{"id":"t739aWRss4Al"}},{"cell_type":"markdown","source":["# ***18. What does model.fit() do? What arguments must be given***"],"metadata":{"id":"hHhhJT-5tSde"}},{"cell_type":"markdown","source":["Ans= In Python’s scikit-learn library, the method model.fit() is used to train a machine learning model on a dataset. When you call fit(), the model learns the relationship between the input features and the target variable by adjusting its internal parameters (like weights and biases) to minimize the loss or error. Essentially, this is the step where the model “fits” itself to the data so it can make accurate predictions later.\n","\n","The main arguments required for model.fit() are:\n","\n","X – The input features (independent variables). This is usually a 2-dimensional array or DataFrame where each row represents a sample and each column represents a feature.\n","\n","y – The target variable (dependent variable) that the model is supposed to predict. This can be a 1-dimensional array for regression or classification tasks."],"metadata":{"id":"vIlrJqoatadZ"}},{"cell_type":"markdown","source":["# ***19. What does model.predict() do? What arguments must be given***"],"metadata":{"id":"iEazX-p0tgvE"}},{"cell_type":"markdown","source":["Ans= In Python’s scikit-learn library, the method model.predict() is used to make predictions using a trained machine learning model. After a model has been trained with model.fit(), it has learned the relationship between input features and the target variable. The predict() method applies this learned knowledge to new, unseen data to estimate the target values.\n","\n","The main argument required for model.predict() is:\n","\n","X – The input features (independent variables) for which you want to make predictions. This should have the same number of features (columns) as the training data.\n","\n","Example:"],"metadata":{"id":"qT8Td-9XtmwG"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","X_train = [[1], [2], [3], [4], [5]]\n","y_train = [2, 4, 6, 8, 10]\n","\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","X_new = [[6], [7]]\n","predictions = model.predict(X_new)\n","print(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_1wYUittuN_","executionInfo":{"status":"ok","timestamp":1760931827040,"user_tz":-330,"elapsed":62,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"26786d21-3f13-4ce3-8004-6f9241015b37"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[12. 14.]\n"]}]},{"cell_type":"markdown","source":["# ***20. What are continuous and categorical variables***"],"metadata":{"id":"VzdJbFlut1Sb"}},{"cell_type":"markdown","source":["Ans= In statistics and machine learning, **variables** represent the characteristics or features of data, and they are broadly classified into **continuous** and **categorical** variables based on the type of values they can take.\n","\n","A **continuous variable** is one that can take **any numerical value within a given range**, including decimal points. These variables are measurable and can vary smoothly. Examples include height, weight, temperature, or time, because they can assume infinitely many values in a range. For instance, a person’s height could be 165.4 cm, 165.45 cm, or 165.456 cm.\n","\n","A **categorical variable**, on the other hand, represents **distinct groups or categories** and usually takes on a limited set of values. These are often qualitative rather than quantitative and describe characteristics or types. Examples include gender (male/female), color (red/blue/green), or car type (sedan/SUV/truck). Categorical variables can be **nominal** (no specific order, like color) or **ordinal** (with a meaningful order, like small, medium, large).\n","\n","\n"],"metadata":{"id":"Tz-tYFTnt9du"}},{"cell_type":"markdown","source":["# ***21. What is feature scaling? How does it help in Machine Learning***"],"metadata":{"id":"u6XNNxkUuJke"}},{"cell_type":"markdown","source":["Ans=  Feature scaling is a preprocessing technique in Machine Learning that standardizes or normalizes the range of independent variables (features) so that they have a similar scale. Since features in a dataset can have vastly different ranges—for example, age might range from 0 to 100, while income could range from 10,000 to 1,000,000—some algorithms may give more importance to features with larger values, which can negatively affect model performance.\n","\n","Feature scaling helps by bringing all features to a comparable range, usually between 0 and 1 (normalization) or by centering them around zero with a standard deviation of 1 (standardization). Common techniques include Min-Max Scaling, Standard Scaling (Z-score normalization), and Max Abs Scaling.\n","\n","It is particularly important for algorithms that rely on distance calculations, such as K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Gradient Descent-based models like linear regression and neural networks. By scaling features, these algorithms can converge faster, avoid bias towards certain features, and produce more accurate predictions"],"metadata":{"id":"Yu6g6bVkuQT1"}},{"cell_type":"markdown","source":["# ***22. How do we perform scaling in Python***"],"metadata":{"id":"_ldJsrBGvGaC"}},{"cell_type":"markdown","source":["Ans= In Python, feature scaling is commonly performed using the sklearn.preprocessing module from the scikit-learn library. It provides tools like StandardScaler and MinMaxScaler to normalize or standardize data so that all features have a similar scale.\n","\n","1. Standardization (Z-score scaling):\n","This method transforms features to have a mean of 0 and standard deviation of 1. It is useful when features have different units or ranges.\n","\n","2. Min-Max Scaling (Normalization):\n","This method scales features to a fixed range, usually 0 to 1, preserving the relationships between values."],"metadata":{"id":"55_SmNEXvJA4"}},{"cell_type":"markdown","source":["# ***23. What is sklearn.preprocessing***"],"metadata":{"id":"Q3VXreBuvVxe"}},{"cell_type":"markdown","source":["Ans= In Python, **`sklearn.preprocessing`** is a module in the **scikit-learn** library that provides tools for **preprocessing and transforming raw data** before it is fed into a machine learning model. Real-world data is often messy, with features having different scales, missing values, or categorical formats. The `preprocessing` module helps make this data **clean, consistent, and suitable for ML algorithms**.\n","\n","It includes functions and classes for tasks such as:\n","\n","* **Scaling numerical features** – `StandardScaler`, `MinMaxScaler`, `MaxAbsScaler`\n","* **Encoding categorical variables** – `LabelEncoder`, `OneHotEncoder`\n","* **Normalizing data** – `Normalizer`\n","* **Generating polynomial features** – `PolynomialFeatures`\n","* **Handling missing values** (in combination with other modules)\n","\n","By using `sklearn.preprocessing`, we ensure that all features are **on comparable scales** and in the right format, which improves **training efficiency, model performance, and prediction accuracy**.\n"],"metadata":{"id":"bY7IdmlAvc3e"}},{"cell_type":"markdown","source":["# ***24. How do we split data for model fitting (training and testing) in Python***"],"metadata":{"id":"Sk3zBxY3vkHd"}},{"cell_type":"markdown","source":["Ans= In Python, data for model fitting is typically split into **training and testing sets** using the `train_test_split` function from the **scikit-learn** library. The training set is used to **teach the model** by allowing it to learn patterns and relationships between the input features and the target variable, while the testing set is kept separate to **evaluate the model’s performance** on new, unseen data. This helps ensure that the model can generalize well rather than just memorizing the training data. The `train_test_split` function allows you to specify the proportion of data for testing using the `test_size` parameter and ensures reproducibility with the `random_state` parameter. For example, if `test_size=0.2`, 20% of the data is reserved for testing and 80% for training. Once the data is split, the model is trained on the training set using `model.fit()` and later tested on the testing set to measure its accuracy, error, or other performance metrics. This practice is essential to validate that the machine learning model will perform reliably on real-world data.\n"],"metadata":{"id":"2qHsbm5xvrfx"}},{"cell_type":"markdown","source":["# ***25. Explain data encoding***"],"metadata":{"id":"EndopVZ1v6Tl"}},{"cell_type":"markdown","source":["Ans= **Data encoding** is a preprocessing technique in machine learning used to **convert categorical data into numerical format** so that algorithms can process it effectively. Most machine learning models, especially those based on mathematical computations like linear regression, SVM, or neural networks, cannot work directly with text or non-numeric data. Encoding transforms these categorical variables into numbers while preserving their meaning, allowing the model to learn patterns and relationships from the data.\n","\n","There are several common methods of data encoding:\n","\n","1. **Label Encoding** – Each category is assigned a **unique integer value**. This is suitable for **ordinal data** where the order matters, such as “low,” “medium,” “high.”\n","   *Example:* `Red → 0, Blue → 1, Green → 2`\n","\n","2. **One-Hot Encoding** – Each category is converted into a **binary column** (0 or 1), ensuring no ordinal relationship is assumed. This is used for **nominal data** where categories do not have a meaningful order.\n","   *Example:* For colors Red, Blue, Green, three columns are created: `[1,0,0]` for Red, `[0,1,0]` for Blue, `[0,0,1]` for Green.\n","\n","3. **Ordinal Encoding** – Similar to label encoding but specifically respects the **order or ranking** among categories.\n","\n","4. **Target or Mean Encoding** – Each category is replaced with the **mean of the target variable** for that category. This method is advanced and must be used carefully to avoid overfitting.\n","\n"],"metadata":{"id":"6TjTji5bwBmo"}}]}