{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMW824iuDINcwSScMaP3IsV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ***1. What is a Decision Tree, and how does it work in the context of classification?***"],"metadata":{"id":"hVsqnJ9B9QfS"}},{"cell_type":"markdown","source":["Ans= A Decision Tree is a supervised machine learning algorithm used mainly for classification tasks, and it works by learning simple decision rules from data. In data science, it is considered one of the most intuitive and interpretable models because it closely resembles human decision-making. A decision tree represents data in a tree-like structure where each internal node contains a question based on a feature, each branch represents the possible outcome of that question, and each leaf node represents the final class. The algorithm starts from the root node, which is chosen by selecting the feature that best splits the dataset into pure groups. Purity is measured using metrics like Gini Impurity or Entropy, and the split that gives the highest Information Gain becomes the choice. After this, the dataset is recursively divided into smaller subsets, and at each stage the best feature is selected for further splitting. This recursive process continues until either all samples in a node belong to the same class, or no features remain, or a stopping rule such as maximum depth is reached. This helps prevent the model from overfitting, which is a common problem with decision trees.\n","\n","For prediction, a new data point simply travels from the root down through the branches by following the decision rules until it reaches a leaf node, where the final class label is assigned. Decision Trees are widely used because they are easy to understand, require no feature scaling, and can handle both numerical and categorical data. However, they can become too complex and unstable if not properly pruned or regularized. In real-world scenarios, decision trees are used in fields like finance for loan approval and fraud detection, in healthcare for medical diagnosis, in marketing for customer segmentation, and in email filtering for spam detection. Overall, a decision tree is a powerful classification tool that provides clear rules for decision making and forms the foundation of advanced ensemble methods like Random Forests and XGBoost, making it very important in the work of a data scientist."],"metadata":{"id":"8U3A9WdB9UaA"}},{"cell_type":"markdown","source":["# ***2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?***\n"],"metadata":{"id":"EQ2pA49V9cmZ"}},{"cell_type":"markdown","source":["Ans= In a Decision Tree, impurity measures such as Gini Impurity and Entropy are used to decide how good a split is at separating different classes. Gini Impurity measures how often a randomly chosen sample from a node would be incorrectly classified if it were labeled according to the class distribution of that node. A node with samples from only one class has a Gini value of 0, meaning it is perfectly pure, while nodes with mixed classes have higher Gini values. Entropy, on the other hand, comes from information theory and measures the amount of disorder or uncertainty in a node. A pure node has an entropy of 0, whereas a node with completely mixed classes has higher entropy. Decision Trees use these impurity values to evaluate splits: the algorithm tests each possible feature and threshold, calculates the impurity after the split, and chooses the feature that produces the maximum reduction in impurity. This reduction is called Information Gain when using entropy. In simple terms, a good split is one that separates the classes as clearly as possible, resulting in child nodes with lower impurity. Thus, both Gini and Entropy guide the tree to create branches that make the data more organized and improve classification accuracy. While Gini is slightly faster to compute and usually preferred in CART-based trees, Entropy tends to create more balanced splits. In both cases, their purpose is the same: to help the tree choose the most informative feature at each step, ensuring effective and meaningful learning."],"metadata":{"id":"-rylmimR9hm3"}},{"cell_type":"markdown","source":["# ***3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.***"],"metadata":{"id":"XofzflOF9pb4"}},{"cell_type":"markdown","source":["Ans= In Decision Trees, pre-pruning and post-pruning are two techniques used to prevent overfitting by controlling the size and complexity of the tree. Pre-pruning, also called early stopping, stops the tree from growing too deep during the training process. This is done by applying conditions such as maximum depth, minimum samples required to split, or minimum samples in a leaf. As soon as any of these conditions are met, the tree stops expanding further. The practical advantage of pre-pruning is that it reduces training time and makes the model faster, which is useful when working with large datasets.\n","\n","Post-pruning, on the other hand, allows the tree to grow fully first and then trims unnecessary branches afterward. The idea is to remove nodes that provide little improvement in accuracy. This is usually done by checking performance on a validation dataset and pruning those branches that cause overfitting. The main practical advantage of post-pruning is that it improves generalization, resulting in a more accurate and stable model on unseen data. In simple words, pre-pruning focuses on controlling growth early, while post-pruning focuses on correcting overgrowth later."],"metadata":{"id":"gS2nqTaZ98fG"}},{"cell_type":"markdown","source":["# ***4. What is Information Gain in Decision Trees, and why is it important for choosing the best split***"],"metadata":{"id":"H8NT8jsK-Il2"}},{"cell_type":"markdown","source":["Ans= Information Gain is a measure used in Decision Trees to determine how useful a feature is for separating the classes in a dataset. It is based on the concept of Entropy, which represents the amount of randomness or disorder in the data. When a dataset is split using a particular feature, the entropy of the resulting child nodes usually decreases because the classes become more organized. Information Gain is calculated as the difference between the entropy of the parent node and the weighted entropy of the child nodes after the split. If a feature reduces entropy significantly, it means that the feature provides a lot of information about the class labels. Therefore, a high Information Gain indicates a good split, while a low Information Gain indicates a less effective split. This is important because Decision Trees rely on Information Gain to select the best feature at each step in the tree-building process. By choosing the split with the highest Information Gain, the tree becomes more accurate, learns better class separation, and reduces impurity more effectively. In short, Information Gain guides the tree to grow in the most informative direction and helps avoid unnecessary or weak splits."],"metadata":{"id":"aM5Hj3XT-LNX"}},{"cell_type":"markdown","source":["# ***5.  What are some common real-world applications of Decision Trees, and what are their main advantages and limitations***"],"metadata":{"id":"7THPkasd-Rbf"}},{"cell_type":"markdown","source":["Ans= Decision Trees are widely used in many real-world applications because they are easy to understand and interpret. In finance, they are used for credit scoring, loan approval, fraud detection, and risk assessment, helping banks make quick decisions based on customer data. In healthcare, decision trees assist doctors in diagnosing diseases by analyzing symptoms and test results. In marketing, companies use them for customer segmentation, predicting customer churn, and designing targeted advertising strategies. In manufacturing, they help in fault detection, quality control, and identifying causes of machine failures. Decision Trees are also used in telecom for predicting service upgrades, and in email filtering systems to classify messages as spam or not spam.\n","\n","The main advantages of decision trees include their simplicity, as they can be easily visualized and explained; their ability to handle both numerical and categorical data; and the fact that they do not require scaling or normalization. They can also capture non-linear relationships effectively. However, they also have limitations—Decision Trees are prone to overfitting, especially when they grow too deep, and can become overly complex. They are also sensitive to small changes in data, which can result in a completely different tree. Additionally, single decision trees may not provide the highest accuracy compared to ensemble methods like Random Forests or Gradient Boosting. Overall, while extremely useful and interpretable, decision trees must be carefully pruned and tuned to achieve good performance."],"metadata":{"id":"tcRNefQq-XFm"}},{"cell_type":"markdown","source":["# ***6.Write a Python program to:***\n","# ***● Load the Iris Dataset***\n","# ***● Train a Decision Tree Classifier using the Gini criterion***\n","# ***● Print the model’s accuracy and feature importances***"],"metadata":{"id":"QRfXkf84-fx3"}},{"cell_type":"code","source":["\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","model = DecisionTreeClassifier(criterion='gini', random_state=42)\n","model.fit(X_train, y_train)\n","\n","\n","y_pred = model.predict(X_test)\n","\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Model Accuracy:\", accuracy)\n","\n","\n","print(\"Feature Importances:\")\n","for name, importance in zip(iris.feature_names, model.feature_importances_):\n","    print(f\"{name}: {importance}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8wVE1KK_hzY","executionInfo":{"status":"ok","timestamp":1763453951885,"user_tz":-330,"elapsed":26,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"01558d22-ab06-408b-af17-6e457379d35c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.0\n","Feature Importances:\n","sepal length (cm): 0.0\n","sepal width (cm): 0.01911001911001911\n","petal length (cm): 0.8932635518001373\n","petal width (cm): 0.08762642908984374\n"]}]},{"cell_type":"markdown","source":["# ***7. Write a Python program to:***\n","# ***● Load the Iris Dataset***\n","# ***● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to*** ***a fully-grown tree***"],"metadata":{"id":"GuHUyH4J_npV"}},{"cell_type":"code","source":["\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n","limited_tree.fit(X_train, y_train)\n","\n","full_tree = DecisionTreeClassifier(random_state=42)\n","full_tree.fit(X_train, y_train)\n","\n","limited_pred = limited_tree.predict(X_test)\n","full_pred = full_tree.predict(X_test)\n","\n","limited_accuracy = accuracy_score(y_test, limited_pred)\n","full_accuracy = accuracy_score(y_test, full_pred)\n","\n","print(\"Accuracy with max_depth=3 :\", limited_accuracy)\n","print(\"Accuracy of fully-grown tree:\", full_accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hI81BVB_xId","executionInfo":{"status":"ok","timestamp":1763453142845,"user_tz":-330,"elapsed":25,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"d8b491f3-4fac-4d40-af32-06580058ac02"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with max_depth=3 : 1.0\n","Accuracy of fully-grown tree: 1.0\n"]}]},{"cell_type":"markdown","source":["# ***Write a Python program to:***\n","# ***● Load the Boston Housing Dataset***\n","# ***● Train a Decision Tree Regressor***\n","# ***● Print the Mean Squared Error (MSE) and feature importances***"],"metadata":{"id":"h_qsL3OQ_5GF"}},{"cell_type":"code","source":["\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","try:\n","    from sklearn.datasets import load_boston\n","    data = load_boston()\n","    X = data.data\n","    y = data.target\n","    feature_names = data.feature_names\n","    print(\"Loaded Boston Housing dataset using load_boston()\")\n","\n","except:\n","    from sklearn.datasets import fetch_openml\n","    boston = fetch_openml(name='boston', version=1, as_frame=True)\n","    X = boston.data\n","    y = boston.target\n","    feature_names = X.columns\n","    print(\"Loaded Boston Housing dataset using OpenML\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","model = DecisionTreeRegressor(random_state=42)\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Mean Squared Error (MSE):\", mse)\n","\n","print(\"\\nFeature Importances:\")\n","for name, importance in zip(feature_names, model.feature_importances_):\n","    print(f\"{name}: {importance}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipFFqcOkAOSG","executionInfo":{"status":"ok","timestamp":1763453998261,"user_tz":-330,"elapsed":73,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"371e5779-9c7d-4dc7-9081-7a47451c25ca"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded Boston Housing dataset using OpenML\n","Mean Squared Error (MSE): 11.588026315789474\n","\n","Feature Importances:\n","CRIM: 0.05846545229060361\n","ZN: 0.000988919249451643\n","INDUS: 0.009872448809169472\n","CHAS: 0.0002973342835618114\n","NOX: 0.007050562083191356\n","RM: 0.575807411273885\n","AGE: 0.007170198655228184\n","DIS: 0.10962404854314393\n","RAD: 0.001646356693641641\n","TAX: 0.002181112508453187\n","PTRATIO: 0.025042865841170155\n","B: 0.011872990423277916\n","LSTAT: 0.189980299345222\n"]}]},{"cell_type":"markdown","source":["# ***9.  Write a Python program to:***\n","# ***● Load the Iris Dataset***\n","# ***● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV***\n","# ***● Print the best parameters and the resulting model accuracy***"],"metadata":{"id":"fYoBmqaMAQK0"}},{"cell_type":"code","source":["\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","param_grid = {\n","    'max_depth': [2, 3, 4, 5, None],\n","    'min_samples_split': [2, 3, 4, 5, 6]\n","}\n","\n","\n","grid = GridSearchCV(\n","    DecisionTreeClassifier(random_state=42),\n","    param_grid,\n","    cv=5,\n","    scoring='accuracy'\n",")\n","\n","\n","grid.fit(X_train, y_train)\n","\n","\n","print(\"Best Parameters:\", grid.best_params_)\n","\n","\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Model Accuracy with Best Parameters:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALHiMgIqAd2F","executionInfo":{"status":"ok","timestamp":1763454026423,"user_tz":-330,"elapsed":824,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"a05e410c-a6e8-4c59-e224-ea203c93f9a3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n","Model Accuracy with Best Parameters: 1.0\n"]}]},{"cell_type":"markdown","source":["# ***10. Imagine you’re working as a data scientist for a healthcare company that***\n","# **wants to predict whether a patient has a certain disease. You have a large dataset with**\n","# ***mixed data types and some missing values.***\n","# **Explain the step-by-step process you would follow to:**\n","# ***● Handle the missing values***\n","# ***● Encode the categorical features***\n","# ***● Train a Decision Tree model***\n","# ***● Tune its hyperparameters***\n","# ***● Evaluate its performance***\n","# ***And describe what business value this model could provide in the real-world***\n","setting"],"metadata":{"id":"eavdKxhDAfvc"}},{"cell_type":"markdown","source":["1) Handle the missing values\n","\n","Goal: avoid bias/leakage, keep model robust.\n","\n","Steps & options (pick based on how much missingness and why it's missing):\n","\n","Understand missingness first: check % missing per column and whether missingness correlates with target (MCAR / MAR / MNAR).\n","\n","Simple imputation (fast, baseline):\n","\n","Numerical → median (robust to outliers) or mean.\n","\n","Categorical → most_frequent or a new category like \"Missing\".\n","\n","Model-based imputation (better if many missing):\n","\n","KNNImputer (uses neighbors), or IterativeImputer (multivariate).\n","\n","Indicator for missingness: create a binary flag column feature_X_missing for features where missing might be informative (common in healthcare).\n","\n","Do not leak target info: imputation must be done inside cross-validation/pipeline (fit only on training fold).\n","\n","Outliers & scaling: Decision Trees don’t require scaling, but detect outliers for imputation decisions."],"metadata":{"id":"-4YexbwaA0ps"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","num_imputer = SimpleImputer(strategy='median')\n","cat_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n"],"metadata":{"id":"pWwmH1z_B_zy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2) Encode the categorical features\n","\n","Goal: transform categories into numeric form without leaking target info and preserving interpretability.\n","\n","Options & recommendations:\n","\n","Low-cardinality categorical features: OneHotEncoder (sparse output ok). Use drop='if_binary' or handle_unknown='ignore'.\n","\n","High-cardinality features: consider Target/Mean encoding or OrdinalEncoder with caution. If using target encoding, apply it inside CV with smoothing to avoid leakage.\n","\n","Ordered categories (natural order): OrdinalEncoder.\n","\n","Decision Trees can handle ordinal-encoded categories reasonably, but pure label encoding of nominal categories can introduce spurious order — prefer one-hot or target encoding.\n","\n","Always encode inside a ColumnTransformer so transformers are fit only on training data.\n","\n","Example transformers:"],"metadata":{"id":"FZ-soiQ9BBCE"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","preprocessor = ColumnTransformer([\n","    ('num', num_imputer, numeric_cols),\n","    ('cat_onehot', OneHotEncoder(handle_unknown='ignore'), low_card_cat_cols),\n","    ('cat_ord', OrdinalEncoder(), ordinal_cols)\n","])\n"],"metadata":{"id":"ztckh-LzCFtk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3) Train a Decision Tree model\n","\n","Goal: build a baseline, interpretable model.\n","\n","Steps:\n","\n","Use a pipeline that includes preprocessing (imputation + encoding) and the model to avoid leakage.\n","\n","Baseline model: DecisionTreeClassifier(criterion='gini', random_state=42) (or 'entropy').\n","\n","Handle class imbalance (common in disease detection): use class_weight='balanced' or resampling (SMOTE for numeric-heavy data; be careful with categorical mixing).\n","\n","Fit on training data only.\n","\n","Example pipeline:"],"metadata":{"id":"BiJc5Ng6CH8V"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","from sklearn.tree import DecisionTreeClassifier\n","\n","pipeline = Pipeline([\n","    ('preproc', preprocessor),\n","    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n","])\n","pipeline.fit(X_train, y_train)\n"],"metadata":{"id":"Gm-Q7azwCLRj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4) Tune its hyperparameters\n","\n","Goal: prevent overfitting and improve generalization.\n","\n","Important hyperparameters to tune for Decision Trees:\n","\n","max_depth (controls complexity)\n","\n","min_samples_split\n","\n","min_samples_leaf\n","\n","max_features (fraction or number of features to consider at each split)\n","\n","criterion ('gini' or 'entropy')\n","\n","class_weight (if imbalance)\n","\n","Optionally ccp_alpha for cost-complexity pruning (post-pruning)\n","\n","Tuning approach:\n","\n","Use GridSearchCV or RandomizedSearchCV with nested CV or at least k-fold CV (e.g., cv=5).\n","\n","Always wrap the full pipeline in the GridSearchCV so preprocessing + model tuning remain pipeline-contained.\n","\n","For time efficiency, use RandomizedSearchCV then narrow with GridSearchCV.\n","\n","If metric of interest is imbalance-sensitive, use scoring='f1', roc_auc, or a domain-specific cost function.\n","\n","Example grid search:"],"metadata":{"id":"LTzxtfjgCN11"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","  'clf__max_depth': [3, 5, 7, None],\n","  'clf__min_samples_split': [2, 5, 10],\n","  'clf__min_samples_leaf': [1, 2, 4],\n","  'clf__criterion': ['gini', 'entropy'],\n","  'clf__ccp_alpha': [0.0, 0.001, 0.01]\n","}\n","\n","grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n","grid.fit(X_train, y_train)\n","best_model = grid.best_estimator_\n","print(grid.best_params_)\n"],"metadata":{"id":"8DUmvG1bCREO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5) Evaluate its performance\n","\n","Goal: ensure the model is clinically useful and safe.\n","\n","Key evaluation steps & metrics:\n","\n","Hold-out test set: never used in training/tuning. Report metrics on this set.\n","\n","Classification metrics:\n","\n","ROC-AUC (general discrimination)\n","\n","Precision, Recall (Sensitivity) — recall (sensitivity) often critical in disease detection (missing a disease case is costly).\n","\n","Specificity (true negative rate)\n","\n","F1-score if we need balance between precision & recall.\n","\n","Confusion matrix to view FP vs FN trade-offs.\n","\n","Precision-Recall curve for imbalanced problems (gives more insight than ROC for rare disease).\n","\n","Calibration: check whether predicted probabilities match observed rates (use calibration curve/CalibratedClassifierCV).\n","\n","Decision threshold tuning: pick threshold based on business costs (e.g., prefer high recall even if precision drops).\n","\n","Explainability: show feature_importances_ and use local explainers (SHAP or LIME) for per-patient explanations.\n","\n","Statistical significance / confidence intervals for metrics (bootstrap).\n","\n","Fairness checks: ensure performance is equitable across subgroups (age, gender, ethnicity).\n","\n","Robustness: test on temporally-split data or external datasets if available.\n","\n","Example evaluation code:"],"metadata":{"id":"Vtix1OWYCTaO"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve\n","\n","y_pred = best_model.predict(X_test)\n","y_proba = best_model.predict_proba(X_test)[:,1]\n","print(classification_report(y_test, y_pred))\n","print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n"],"metadata":{"id":"3eawfoddCXzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6) Deployment & monitoring considerations (short)\n","\n","Explainability: Decision Trees are interpretable — present simple rules for clinicians. Also use global & local explanations.\n","\n","Integration: integrate model into clinical workflow (EHR), with UI for clinicians showing probability + explanation.\n","\n","Threshold policy: specify action on positive prediction (further tests, specialist review).\n","\n","Monitoring: continuously monitor drift (data distribution, performance), retrain on new labeled data periodically.\n","\n","Logging & auditing: store inputs, predictions, outcomes for post hoc review and regulatory needs.\n","\n","Privacy & compliance: follow HIPAA/GDPR as applicable, de-identify data.\n","\n","Human-in-the-loop: use model to triage, not to fully automate diagnoses — final human decision required."],"metadata":{"id":"MJYGSx0nCb9z"}},{"cell_type":"markdown","source":["7) Business value in the real-world healthcare setting\n","\n","Early detection & improved outcomes: catching disease earlier can increase treatment success and lower mortality.\n","\n","Prioritization & triage: helps clinicians prioritize patients for follow-up tests or urgent care, improving workflow efficiency.\n","\n","Cost savings: reduces unnecessary tests by flagging likely negatives and directing resources to likely positives.\n","\n","Operational planning: forecast demand for specialists, beds, and supplies based on predicted caseloads.\n","\n","Explainable decisions: Decision Trees are easy to interpret — clinicians can see the rule that led to the recommendation, increasing trust and adoption.\n","\n","Risk stratification: helps insurers/providers tailor care plans or screening frequency based on risk.\n","\n","Data-driven policy: aggregate model outputs can inform public health interventions and resource allocation.\n","\n","Caveats on business value:\n","\n","Avoid over-reliance — false positives can cause unnecessary anxiety and cost; false negatives can be dangerous. So align thresholds with business/clinical risk tolerance.\n","\n","Legal/regulatory oversight and clinical validation trials are usually required before production use."],"metadata":{"id":"dDm_PS7QCfr7"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import classification_report, roc_auc_score\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","\n","\n","iris = load_iris()\n","X_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","y = iris.target\n","\n","\n","numeric_cols = iris.feature_names\n","low_card_cat_cols = []\n","ordinal_cols = []\n","\n","num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n","cat_onehot_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","preprocessor = ColumnTransformer([\n","    ('num', num_pipe, numeric_cols),\n","    ('cat1', cat_onehot_pipe, low_card_cat_cols),\n","    ('cat2', Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n","                       ('ord', OrdinalEncoder())]), ordinal_cols)\n","])\n","\n","pipeline = Pipeline([\n","    ('preproc', preprocessor),\n","    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n","])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42, stratify=y)\n","\n","param_grid = {\n","  'clf__max_depth': [3, 5, 7, None],\n","  'clf__min_samples_leaf': [1, 2, 4],\n","  'clf__ccp_alpha': [0.0, 0.001, 0.01]\n","}\n","\n","grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n","grid.fit(X_train, y_train)\n","\n","best = grid.best_estimator_\n","print(\"Best params:\", grid.best_params_)\n","y_proba = best.predict_proba(X_test)\n","y_pred = best.predict(X_test)\n","print(classification_report(y_test, y_pred))\n","print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba, multi_class='ovr'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1t6xzZiKBvMl","executionInfo":{"status":"ok","timestamp":1763454068791,"user_tz":-330,"elapsed":6209,"user":{"displayName":"Tumendra Bhalavi","userId":"11018144066799644714"}},"outputId":"2657c943-d8ff-4c7f-a3f8-7accf719d373"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Best params: {'clf__ccp_alpha': 0.0, 'clf__max_depth': 3, 'clf__min_samples_leaf': 4}\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      0.90      0.95        10\n","           2       0.91      1.00      0.95        10\n","\n","    accuracy                           0.97        30\n","   macro avg       0.97      0.97      0.97        30\n","weighted avg       0.97      0.97      0.97        30\n","\n","ROC-AUC: 0.9699999999999999\n"]}]}]}